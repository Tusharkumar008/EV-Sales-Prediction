# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iHY90piK97AnVjel1q0eC5AAOf6W8nqB
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

df= pd.read_csv('Data.csv')

df.head()

region_frequency = df['region'].value_counts()
print(region_frequency)

mode_proportion = df['mode'].value_counts(normalize=True)
print(mode_proportion)

"""**# Convert 'value' and 'percentage' columns to numeric**"""

df['value'] = pd.to_numeric(df['value'], errors='coerce')
df['percentage'] = pd.to_numeric(df['percentage'], errors='coerce')

"""**Check for missing value**"""

df.isnull().sum()

"""**# Drop rows with missing values**"""

df.dropna(inplace=True)

"""**# Plot the trend of EV sales over the years**"""

# Set the style and context for better visual aesthetics
sns.set(style="whitegrid", context="talk")

# Set a larger figure size for better clarity
plt.figure(figsize=(14, 8))

# Customize color palette for the line plot
palette = sns.color_palette("coolwarm", as_cmap=False)

# Plot the trend of EV sales with a smooth line and markers
sns.lineplot(data=df, x='year', y='value', hue='region', palette=palette, linewidth=2.5, marker='o')

# Add a title and labels with enhanced formatting
plt.title('Trend of EV Sales Over the Years', fontsize=18, fontweight='bold')
plt.xlabel('Year', fontsize=14)
plt.ylabel('Sales Value (in Millions)', fontsize=14)

# Customize the legend for better readability
plt.legend(title='Region', title_fontsize='13', fontsize='11', loc='upper left', frameon=True)

# Rotate x-axis labels for better readability if there are many years
plt.xticks(rotation=45)

# Add major and minor gridlines to create box-like structures
plt.grid(True, which='both', axis='both', linestyle='-', linewidth=0.7, color='grey')

# Add horizontal and vertical lines to create box effect
for y in plt.gca().get_yticks():
    plt.axhline(y=y, color='black', linestyle='-', linewidth=0.8)

for x in plt.gca().get_xticks():
    plt.axvline(x=x, color='black', linestyle='-', linewidth=0.8)

# Remove top and right spines for a cleaner look
sns.despine()

# Show the plot
plt.show()

"""**# Correlation heatmap for numeric values**"""

numeric_df = df.select_dtypes(include=[np.number])
plt.figure(figsize=(10, 8))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

"""**Predictive Analysis**"""

year_count = df['year'].count()
year_mean = df['year'].mean()
year_median = df['year'].median()
year_mode = df['year'].mode()[0]
year_range = df['year'].max() - df['year'].min()

print(f"Count: {year_count}, Mean: {year_mean}, Median: {year_median}, Mode: {year_mode}, Range: {year_range}")

value_mean = df['value'].mean()
value_median = df['value'].median()
value_mode = df['value'].mode()[0]
value_range = df['value'].max() - df['value'].min()
value_variance = df['value'].var()
value_std_dev = df['value'].std()

print(f"Mean: {value_mean}, Median: {value_median}, Mode: {value_mode}, Range: {value_range}, Variance: {value_variance}, Std Dev: {value_std_dev}")



# Prepare data for prediction
X = df[['year']]
y = df['value']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mse, r2

# Assuming df has been loaded and contains 'value' (sales), and 'region'

# Step 1: Transform the target variable
threshold = df['value'].median()  # Using median sales as the threshold
df['high_sales'] = (df['value'] > threshold).astype(int)  # 1 for high sales, 0 for low sales

# Step 2: Prepare the data
X = df[['value', 'region']]  # Using value and region as features
y = df['high_sales']  # The binary target variable

# One-hot encode the 'region' feature (if categorical)
column_transformer = ColumnTransformer([('encoder', OneHotEncoder(), ['region'])], remainder='passthrough')

# Transform the feature set
X_transformed = column_transformer.fit_transform(X)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)

# Step 3: Apply Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Step 4: Predict and Evaluate the Model
y_pred = logreg.predict(X_test)

# Print the classification report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

# Plot the confusion matrix
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

# Predict using the logistic regression model
y_pred = logreg.predict(X_test)

# Calculate precision, recall, and F1-score
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print the calculated metrics
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

# You can also display the full classification report
from sklearn.metrics import classification_report
print("Classification Report:\n", classification_report(y_test, y_pred))

import pandas as pd

# Load the data from CSV (use the correct file path)
data = pd.read_csv('Data.csv')

# Filter the data for 'EV sales' parameter and group by vehicle type (mode)
sales_data = data[data['parameter'] == 'EV sales']

# Clean the 'value' column by removing commas and extra periods
# Clean the 'value' column by removing commas and extra periods and replacing empty strings
sales_data['value'] = sales_data['value'].replace({',': '', '\.': ''}, regex=True).replace('', '0').astype(float)

# Prepare data for each vehicle type (Cars, Buses, Vans, etc.)
vehicle_types = sales_data['mode'].unique()

# Create a dictionary to store future predictions for each vehicle type
predictions = {}

# Plot the historical and predicted sales
plt.figure(figsize=(10, 6))

for vehicle in vehicle_types:
    vehicle_data = sales_data[sales_data['mode'] == vehicle]

    # Convert year to numerical format and reshape
    X = vehicle_data['year'].values.reshape(-1, 1)
    y = vehicle_data['value']

    # Train a linear regression model
    model = LinearRegression()
    model.fit(X, y)

    # Predict future sales for the next 5 years
    future_years = np.array(range(vehicle_data['year'].max() + 1, vehicle_data['year'].max() + 6)).reshape(-1, 1)
    future_sales = model.predict(future_years)

    # Store predictions
    predictions[vehicle] = future_sales

    # Plot historical data
    plt.plot(vehicle_data['year'], y, label=f'{vehicle} Historical Sales')

    # Plot future predictions
    plt.plot(future_years, future_sales, '--', label=f'{vehicle} Predicted Sales')

# Customize plot
plt.xlabel('Year')
plt.ylabel('EV Sales')
plt.title('EV Sales Prediction by Vehicle Type')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Output future predictions
predictions